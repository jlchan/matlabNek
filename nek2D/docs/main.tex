%&pdflatex
\documentclass[final,leqno]{siamltex}
%\documentclass[10pt,onecolumn]{article}
\usepackage[top=2cm,bottom=3cm,left=3.5cm,right=3.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs}
\let\proof\relax 
\let\endproof\relax
\usepackage{listings}
\usepackage{array}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage[textsize=footnotesize,color=green]{todonotes}
\usepackage{algorithm, algorithmic}
\usepackage{array}
\usepackage{bm}
\usepackage{tikz}
\usepackage{subfigure}
\usepackage[normalem]{ulem}

%\usepackage{lineno}
%\pagewiselinenumbers
%\usepackage{uselinenofix}


\newcommand{\bs}[1]{\boldsymbol{#1}}

\newcommand{\equaldef}{\stackrel{\mathrm{def}}{=}}

\newcommand{\tablab}[1]{\label{tab:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}

\newcommand{\theolab}[1]{\label{theo:#1}}
\newcommand{\theoref}[1]{\ref{theo:#1}}
\newcommand{\eqnlab}[1]{\label{eq:#1}}
\newcommand{\eqnref}[1]{\eqref{eq:#1}}
\newcommand{\seclab}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{\ref{sec:#1}}
\newcommand{\lemlab}[1]{\label{lem:#1}}
\newcommand{\lemref}[1]{\ref{lem:#1}}

\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\nor}[1]{\left\| #1 \right\|}
\newcommand{\snor}[1]{\left| #1 \right|}
\newcommand{\LRp}[1]{\left( #1 \right)}
\newcommand{\LRs}[1]{\left[ #1 \right]}
\newcommand{\LRa}[1]{\left\langle #1 \right\rangle}
\newcommand{\LRc}[1]{\left\{ #1 \right\}}
\newcommand{\LRb}[1]{\left| #1 \right|}

\newcommand{\tanbui}[2]{\textcolor{blue}{\sout{#1}} \textcolor{red}{#2}}
\newcommand{\Grad} {\ensuremath{\nabla}}
\newcommand{\Div} {\ensuremath{\nabla\cdot}}
\newcommand{\Nel} {\ensuremath{{N^\text{el}}}}
\newcommand{\jump}[1] {\ensuremath{\LRs{\!\left[#1\right]\!}}}
\newcommand{\uh}{\widehat{u}}
\newcommand{\fnh}{\widehat{f}_n}
\renewcommand{\L}{L^2\LRp{\Omega}}
\newcommand{\pO}{\partial\Omega}
\newcommand{\Gh}{\Gamma_h}
\newcommand{\Gm}{\Gamma_{-}}
\newcommand{\Gp}{\Gamma_{+}}
\newcommand{\Go}{\Gamma_0}
\newcommand{\Oh}{\Omega_h}

\newcommand{\eval}[2][\right]{\relax
  \ifx#1\right\relax \left.\fi#2#1\rvert}

\def\etal{{\it et al.~}}

\newcommand{\vect}[1]{\ensuremath\boldsymbol{#1}}
\newcommand{\tensor}[1]{\underline{\vect{#1}}}
\newcommand{\del}{\Delta}
\let\grad\relax
\newcommand{\grad}{\nabla}
\newcommand{\curl}{\grad \times}
\renewcommand{\div}{\grad \cdot}
\newcommand{\ip}[1]{\left\langle #1 \right\rangle}
\newcommand{\eip}[1]{a\left( #1 \right)}
\newcommand{\pd}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\pdd}[2]{\frac{\partial^2#1}{\partial#2^2}}

\newcommand{\circone}{\ding{192}}
\newcommand{\circtwo}{\ding{193}}
\newcommand{\circthree}{\ding{194}}
\newcommand{\circfour}{\ding{195}}
\newcommand{\circfive}{\ding{196}}

\def\arr#1#2#3#4{\left[
\begin{array}{cc}
#1 & #2\\
#3 & #4\\
\end{array}
\right]}
\def\vecttwo#1#2{\left[
\begin{array}{c}
#1\\
#2\\
\end{array}
\right]}
\def\vectthree#1#2#3{\left[
\begin{array}{c}
#1\\
#2\\
#3\\
\end{array}
\right]}
\def\vectfour#1#2#3#4{\left[
\begin{array}{c}
#1\\
#2\\
#3\\
#4\\
\end{array}
\right]}

%\newtheorem{proposition}{Proposition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}

\newcommand{\G} {\Gamma}
\newcommand{\Gin} {\Gamma_{in}}
\newcommand{\Gout} {\Gamma_{out}}
\newcommand{\insub}{{\rm in}}
\newcommand{\outsub}{{\rm out}}

\newtheorem{remark}{Remark}

\title{Notes on preconditioning for implicit CD systems}
\begin{document}

\maketitle

\section{Introduction}

A common approach to the numerical solution of the Stokes equations
\begin{align*}
\pd{u}{t} + u\cdot\grad u + \grad p - \nu \Delta u &= f \\
\div u &= 0
\end{align*}
is to employ a splitting or projection method, where we 
\begin{enumerate}
\item compute an intermediate velocity $u^*$ by solving $$\frac{(u^*-u^k)}{dt} - \nu\Delta u^* = f(t^k)$$
\item correct the velocity by solving 
$$\begin{cases}
\frac{(u^{k+1}-u^*)}{dt} + \nabla p^k &= 0\\
\nabla \cdot u^{k+1} &= 0\\
u^{k+1}\cdot n &= 0 \quad \text{on the boundary}
\end{cases}$$
\end{enumerate}
This above step is the reason splitting schemes are referred to as projection methods - the weak form of (2) can be interpreted as the scaled $L^2$ projection of $u^*$ onto a divergence-free $u^{k+1}$ (after multiplying by a test function, $p$ can then be viewed as a Lagrange multiplier to enforce the divergence free constraint).  Since pressure is undetermined in the second step, we need to produce an approximation for $p$.  Based on the assumption that $\nabla \cdot u^{k+1}$ must be $0$, we can take the divergence of the second equation to get $$\nabla \cdot \frac{u^*}{dt} + \nabla\cdot \nabla p^k = 0,$$ which yields a Poisson equation for $p$ given $u^*$. 

The overall scheme is then 
\begin{enumerate}
\item Solve for $u^*$ through $$\frac{(u^*-u^k)}{dt} - \nu\Delta u^* = f(t^k)$$
\item  Solve for $p$ through $$\Delta p = \frac{1}{dt}\nabla \cdot u^*$$
\item Solve for $u^{k+1}$ through $$\frac{(u^{k+1}-u^*)}{dt} + \nabla p^k = 0.$$
\end{enumerate}
Note that both steps 2 and 3 involve the solution of a Poisson and $L^2$-projection problem, respectively, both of which involve symmetric, positive-definite operators which can be solved very efficiently in a scalable fashion.  However, as step 1 involves the solution of a non-symmetric convection-diffusion system, the full system is often eschewed in favor of a second splitting, where the convective term is updated explicitly, and diffusion accounted for implicitly.  The solution of the full Navier-Stokes equations is then solved via the series of steps
\begin{align*}
\pd{u}{t} + N(u) + \grad p - \nu \Delta u &= f \\
\div u &= 0
\end{align*}
can be done through the following three steps:
\begin{enumerate}
\item Solve for $u^*$ through $$\frac{(u^*-u^k)}{dt} = - N(u(t^k)) + f(t^k)$$
\item  Solve for $p$ through $$-\Delta p = \frac{1}{dt}\nabla \cdot u^* + \div(N(u)-f)$$
\item Solve for $u^{k+1}$ through $$\frac{(u^{k+1}-u^*)}{dt} - \nu\Delta u^{k+1} = -\nabla p^k = 0.$$
\end{enumerate}
A particularly restrictive aspect of this solution method is the CFL condition for spectral finite elements
\[
dt < O\LRp{\LRb{u}\frac{h}{N^2}},
\]
which results from the $O(1/N^2)$ minimum spacing between the GLL nodes of order $N$ on a given element.  For high $N$ ($N>20$ and higher), the explicit CFL condition can be incredibly restrictive.  The solution of the first equation is the inversion of a mass matrix, which is diagonal for SEM and essentially free.  The solution to the second and third steps require a Poisson and reaction-diffusion solve, respectively, which can be solved in a very efficient manner matrix-free manner.  

Our aim is to consider instead the direct solution of the convection-diffusion system, which we hope will allow us to loosen the CFL restriction.  

Pretend the ratio of desired CFL to current CFL is $r$.
\begin{enumerate}
\item A single timestep costs 2 elliptic solves - assume each takes $C_{mv}$ matrix-vector multiplications and preconditioner cost $C_{pre}$.  
\item $r(C_{mv} + C_{pre})$ is the cost of explicitly time-integrating over a single timestep.  The cost of one implicit solve must be less than this.  
\item Do a higher order time integration scheme to make up for difference in time accuracy, or quantify when it doesn't matter.  
\item How to take into account memory cost?
\end{enumerate}

\section{Scalar convection-diffusion}

We investigate first the scalar convection-diffusion-reaction equation with homogeneous Dirichlet boundary condtions.  
\begin{align*}
\alpha u + \div(\beta u) - \epsilon \Delta u &= f\\
\left.u\right|_{\Gamma} = 0,
\end{align*}
and preconditioning strategies for systems resulting from high order spectral element discretizations of this equation.  For most of the examples given, $\alpha = 1$ unless specified otherwise.  

\subsection{1D experiments}

Effective preconditioning of convection-diffusion systems has often exploited the physics of convection-dominated problems.  The relevant physics are often encapsulated in the Greens function, which has been effectively used in the preconditioning of systems resulting from discretizations of convection-diffusion problems.  The idea put forth in \cite{Loghin97,Loghin99} is that the support of the Green's function in the convection-dominated limit is purely upstream.  Under a ``with-the-flow'' ordering of the degrees of freedom, the matrix inverse becomes lower triangular.  Loghin and Wathen take advantage of this by using a Gauss-Seidel preconditioner on both uniform and adaptive meshes of linear and bilinear elements.  

\begin{figure}
\subfigure[Greens function for $\epsilon = 1.0$]{\includegraphics[width=.47\textwidth]{g_eps1.eps}}
\subfigure[Greens function for $\epsilon = 10^{-2}$]{\includegraphics[width=.47\textwidth]{g_eps100.eps}}
\caption{Greens functions for diffusive/convective regimes.}
\end{figure}

A requirement for use of the Gauss-Seidel preconditioners for the convection-diffusion problem is a discretization that is stable in the convective limit; to this end, they employ SUPG stabilization in all their examples.  

\subsubsection{Resolved meshes and higher order elements}

\begin{figure}
\subfigure[Under-resolved convergence for $\epsilon = 10^{-6}$]{\includegraphics[width=.47\textwidth]{GS_underResolved.eps}}
\subfigure[Resolved convergence for $\epsilon = 10^{-2}$]{\includegraphics[width=.47\textwidth]{GS_resolved.eps}}
%\subfigure[GMRES convergence]{\includegraphics[width=.47\textwidth]{GS_higherOrder.eps}}
\caption{GMRES convergence for resolved and under-resolved linear meshes.}
\label{fig:resolved_GS}
\end{figure}

Unfortunately, the Gauss-Seidel preconditioner relies on the fact that the support of the convection-diffusion Green's function in the cross and downstream directions decays to zero over a distance of roughly $\epsilon$; this implies that, for resolved meshes where $h\approx \epsilon$, a ``with-the-flow'' preconditioner does not work as effectively as with under-resolved meshes.  Figure~\ref{fig:resolved_GS} shows GMRES\footnote{Restart is taken every 10 iterations.} convergence in these two cases; for $h/\epsilon < 1$, convergence is relatively fast; for $h/\epsilon = O(1)$ or larger, the convergence rate suffers.  

\begin{figure}
\subfigure[Under-resolved convergence for $\epsilon = 10^{-6}$]{\includegraphics[width=.47\textwidth]{GS_HO_underResolved.eps}}
\subfigure[Resolved convergence for $\epsilon = 10^{-2}$]{\includegraphics[width=.47\textwidth]{GS_HO_resolved.eps}}
%\subfigure[GMRES convergence]{\includegraphics[width=.47\textwidth]{GS_higherOrder.eps}}
\caption{Greens functions for diffusive/convective regimes.}
\label{fig:HO_GS}
\end{figure}

Higher order elements display similarly poor convergence; they suffer poor behavior with downwind Gauss-Seidel preconditioning both for resolved and underresolved meshes.  This is due in part to the suboptimal nature of the SUPG constant chosen -- we take $\tau = \max\{h/(2N)-\epsilon,0\}$, which, while used often, is not an optimal value.  

\subsubsection{Schur complement downwind Gauss-Seidel}

An advantage of higher order discretizations in 1D is that higher order degrees of freedom can be eliminated, yielding a system only in terms of vertex degrees of freedom.  In matrix form, assume that we order our degrees of freedom by higher order degrees of freedom first, then by vertex degrees of freedom.  The resulting system
\[
\arr{A}{B}{C}{D}\vecttwo{u_i}{u_e} = \vecttwo{f}{g}
\]
can be reduced to the solution of the Schur complement system
\[
(D-CA^{-1}B)u_e = g - CA^{-1}f.
\]
For nodal basis functions, the support of higher order degrees of freedom is local, making $A$ block diagonal.  

\begin{figure}
\subfigure[Green's function for $\epsilon = 10^{-2}$]{\includegraphics[width=.47\textwidth]{g_full_schur.eps}}
\subfigure[ GMRES convergence for $\epsilon = 10^{-2}$]{\includegraphics[width=.47\textwidth]{gmres_full_schur.eps}}
\caption{Greens functions/GMRES convergence for full and Schur complement systems.}
\label{fig:gmres_HO_GS}
\end{figure}

An advantage of this method is that Gauss-Seidel preconditioning works for high order resolved discretizations, specifically the case where $h/N > \epsilon$, provided that $h > \epsilon$.  Figure~\ref{fig:gmres_HO_GS} demonstrates this for a non-stabilized discretization (SUPG stabilization behaves similarly, though static condensation itself has been observed to have stabilizing properties \cite{Cai14}), with GMRES convergence restored for the Schur complement system.  The Greens function for the statically condensed system similarly loses its upstream support, indicating that the inverse of the Schur complement system recovers its lower-triangular nature.  

In all cases, $K = 16$, and $N=4$.

\subsection{AGMG in 1D}

Aggregation multigrid is a purely algebraic multigrid method based on the magnitude of the entries in the resulting system \cite{notay2010aggregation}.  

\begin{figure}
\subfigure[Green's function approximation, damped Jacobi]{\includegraphics[width=.47\textwidth]{g_agmg_DJ.eps}}
\subfigure[Green's function approximation, Gauss-Seidel]{\includegraphics[width=.47\textwidth]{g_agmg_GS.eps}}
\caption{Greens functions approximations.}
\label{fig:greens_AGMG}

\end{figure}
\begin{figure}
\subfigure[ GMRES with damped Jacobi smoothing]{\includegraphics[width=.47\textwidth]{gmres_agmg_DJ.eps}}
\subfigure[ GMRES with Gauss-Seidel smoothing]{\includegraphics[width=.47\textwidth]{gmres_agmg_GS.eps}}
\caption{Greens functions/GMRES convergence for full and Schur complement systems.}
\label{fig:gmres_AGMG}
\end{figure}

Both standard and symmetric Gauss-Seidel give results similar to Wathen's preconditioner.  We note also that, unlike Wathen's preconditioner, the behavior of AGMG with remains relatively unchanged under a reordering of dofs.  

In all cases, $K = 16$, and $N=4$.  

\subsection{2D experiments}

In higher dimension 2D, the above analysis changes somewhat; Wathen's preconditioner becomes a block Gauss-Seidel method, where each block corresponds roughly to a collection of nodal values on differing streamlines.  

We consider now $\alpha > 1$; practically speaking, we are interested in preconditioning implicit-in-time systems where $\alpha = dt^{-1}$.  In particular, we are interested in solving timestepping systems with CFL constants $C=10-20$, where 
\[
dt < C\LRb{u}\frac{h}{N^2}.
\]
Comparative CFL conditions for 
\begin{align*}
\alpha u + \div(\beta u) - \epsilon \Delta u &= f\\
\left.u\right|_{\Gamma} = 0,
\end{align*}

Figure~\ref{fig:agmg_ref} shows AGMG behavior under both damped Jacobi and symmetric Gauss-Seidel smoothing for the elliptic problem when $\beta = (0,0)$ (i.e. implicit diffusion).  The parameters are varied as a function of mesh size $h$ and order $N$; the $\alpha$ is set as $\alpha = C h/N^2$, where $C$ is the desired CFL constant (varied between $1, 5, 10$ in these experiments).  $\epsilon$ is set as 
\[
\epsilon = C_\epsilon h/N,
\]
where $C_\epsilon = .2, 1, 5$, corresponding to slightly over-resolved, resolved, and slightly under-resolved regions.  $h$ is set to $1/4, 1/8, 1/16, 1/32$, and $N$ is set to relatively high values $N = 8, 10, 12, 14, 16$.   

While AGMG doesn't show parameter-independent behavior, it does show relatively good independence on both $h$ and $N$ for fixed parameters.  
\begin{figure}
\subfigure[ GMRES with damped Jacobi smoothing]{\includegraphics[width=.49\textwidth]{ref_agmg_cd_DJ.eps}}
\subfigure[ GMRES with symmetric Gauss-Seidel smoothing]{\includegraphics[width=.49\textwidth]{ref_agmg_cd_SymGS.eps}}
\caption{Iterations to convergence for implicit diffusion under damped Jacobi and symmetric Gauss-Seidel smoothing.}
\label{fig:agmg_ref}
\end{figure}
In Figure~\ref{fig:agmg_res}, the convection vector is set to $\beta = (1,0)$.  The behavior of AGMG is observed to depend more heavily on the parameters 

Figure~\ref{fig:agmg_ratio} shows the ratio of the number of iterations to convergence for implicit convection-diffusion compared to implicit diffusion in order to illustrate the impact of nonsymmetry introduced by nonzero convection.  

The addition of improved coarsening strategies for convection-diffusion problems may further improve performance \cite{notay2012aggregation}.
\begin{figure}
\subfigure[ GMRES with damped Jacobi smoothing]{\includegraphics[width=.49\textwidth]{res_agmg_cd_DJ.eps}}
\subfigure[ GMRES with symmetric Gauss-Seidel smoothing]{\includegraphics[width=.49\textwidth]{res_agmg_cd_SymGS.eps}}
\caption{Iterations to convergence for implicit convection-diffusion under damped Jacobi and symmetric Gauss-Seidel smoothing.}
\label{fig:agmg_res}
\end{figure}

\begin{figure}
\subfigure[ GMRES with damped Jacobi smoothing]{\includegraphics[width=.49\textwidth]{ratio_agmg_cd_DJ.eps}}
\subfigure[ GMRES with symmetric Gauss-Seidel smoothing]{\includegraphics[width=.49\textwidth]{ratio_agmg_cd_SymGS.eps}}
\caption{Ratio of iterations between convection-diffusion and elliptic problems under damped Jacobi and symmetric Gauss-Seidel smoothing.}
\label{fig:agmg_ratio}
\end{figure}

\begin{enumerate}
\item Document current AGMG verification ($h$ and $N$ independence)
\item Compare AGMG/Wathen downwind GS
\item Figure out if we can do static condensation case/how to do matrix-free condensation?  
\item Implement $A+A^T$ AGMG coarsening
\end{enumerate}


\bibliographystyle{unsrt}
\bibliography{main}

\end{document}